#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
inference.py
------------
EÄŸitilmiÅŸ model ile tahmin yapma (inference) scripti.
Yeni MRI gÃ¶rÃ¼ntÃ¼leri iÃ§in demans seviyesi tahmini yapar.

KullanÄ±m:
    python3 inference.py --model path/to/model.pkl --image path/to/image.jpg
    python3 inference.py --model xgboost_latest.pkl --batch path/to/images/
"""

import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import argparse
import pickle
import json
import numpy as np
import pandas as pd
from typing import Union, List, Dict, Optional
from multiprocessing import Pool, cpu_count
from functools import partial
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler

# GÃ¶rÃ¼ntÃ¼ iÅŸleme iÃ§in
sys.path.insert(0, str(Path(__file__).parent.parent / "goruntu_isleme"))
from goruntu_isleyici import GorselIsleyici
from ayarlar import PROJE_KOK
from goruntu_isleme.ayarlar import SCALING_METODU, CSV_DOSYA_ADI

from ayarlar import MODELS_KLASORU
from scipy import ndimage
from scipy.stats import skew, kurtosis


def _batch_tahmin_wrapper(goruntu_yolu: Path, model_yolu: Path) -> Dict:
    """âš¡ Paralel batch tahmin iÃ§in wrapper fonksiyon."""
    try:
        inference = ModelInference(model_yolu)
        return inference.tahmin_yap(goruntu_yolu, detayli=False)
    except Exception as e:
        return {
            'dosya': str(goruntu_yolu),
            'hata': str(e)
        }


class ModelInference:
    """EÄŸitilmiÅŸ model ile tahmin yapma sÄ±nÄ±fÄ±."""
    
    def __init__(self, model_yolu: Union[str, Path]):
        """
        Inference nesnesini baÅŸlat.
        
        Args:
            model_yolu: EÄŸitilmiÅŸ model dosyasÄ±nÄ±n yolu (.pkl)
        """
        self.model_yolu = Path(model_yolu)
        self.scaler = None
        self.scaler_columns: List[str] = []
        self.selected_features: Optional[List[str]] = None
        self.feature_names: Optional[List[str]] = None
        
        if not self.model_yolu.exists():
            # MODELS_KLASORU iÃ§inde ara
            alternatif = MODELS_KLASORU / self.model_yolu.name
            if alternatif.exists():
                self.model_yolu = alternatif
            else:
                raise FileNotFoundError(f"Model bulunamadÄ±: {model_yolu}")
        
        # Modeli yÃ¼kle
        self._model_yukle()
        # Ã–lÃ§ekleyiciyi hazÄ±rla (eÄŸitim verisinden)
        self._hazirla_scaler()
        
        # GÃ¶rÃ¼ntÃ¼ iÅŸleyici
        self.isleyici = GorselIsleyici()
        
        # SÄ±nÄ±f isimleri
        self.sinif_isimleri = {
            0: "NonDemented (SaÄŸlÄ±klÄ±)",
            1: "VeryMildDemented (Ã‡ok Hafif Demans)",
            2: "MildDemented (Hafif Demans)",
            3: "ModerateDemented (Orta Seviye Demans)"
        }
        
        # âš¡ Paralel iÅŸlem iÃ§in
        self.n_jobs = max(1, cpu_count() - 1)
    
    def _model_yukle(self):
        """Modeli ve metadata'sÄ±nÄ± yÃ¼kle."""
        print(f"\nğŸ“¦ Model yÃ¼kleniyor: {self.model_yolu.name}")
        
        # Pickle model yÃ¼kle
        with open(self.model_yolu, 'rb') as f:
            self.model = pickle.load(f)
        print(f"   âœ“ Model yÃ¼klendi")
        
        # Metadata yÃ¼kle (varsa)
        metadata_yolu = self.model_yolu.with_suffix('.json')
        if metadata_yolu.exists():
            with open(metadata_yolu, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            print(f"   âœ“ Metadata yÃ¼klendi")
            print(f"   â„¹ï¸  Model Tipi: {self.metadata.get('model_tipi', 'N/A')}")
            print(f"   â„¹ï¸  EÄŸitim Tarihi: {self.metadata.get('tarih', 'N/A')}")
            self.feature_names = self.metadata.get('feature_names')
            self.selected_features = self.metadata.get('selected_features')
            
            # Metrikler varsa gÃ¶ster
            if 'metrikler' in self.metadata:
                metriks = self.metadata['metrikler']
                acc = metriks.get('accuracy')
                if isinstance(acc, (int, float)):
                    print(f"   â„¹ï¸  Test Accuracy: {acc:.4f}")
                else:
                    print(f"   â„¹ï¸  Test Accuracy: {acc}")
        else:
            self.metadata = {}
            print(f"   âš ï¸  Metadata bulunamadÄ±")

    def _hazirla_scaler(self):
        """EÄŸitim verisinden Ã¶lÃ§ekleyiciyi hazÄ±rla."""
        try:
            raw_csv = PROJE_KOK / "goruntu_isleme" / "cikti" / CSV_DOSYA_ADI
            if not raw_csv.exists():
                print(f"   âš ï¸  Ã–lÃ§ekleyici hazÄ±rlanamadÄ± (CSV yok): {raw_csv}")
                return
            
            df = pd.read_csv(raw_csv)
            kategorik = ['dosya_adi', 'sinif', 'etiket', 'tam_yol']
            self.scaler_columns = [c for c in df.columns if c not in kategorik]
            
            if SCALING_METODU == "minmax":
                scaler = MinMaxScaler()
            elif SCALING_METODU == "robust":
                scaler = RobustScaler()
            elif SCALING_METODU == "standard":
                scaler = StandardScaler()
            elif SCALING_METODU == "maxabs":
                scaler = MaxAbsScaler()
            else:
                print(f"   âš ï¸  Bilinmeyen SCALING_METODU: {SCALING_METODU}")
                return
            
            scaler.fit(df[self.scaler_columns])
            self.scaler = scaler
            if not self.feature_names:
                self.feature_names = self.scaler_columns
            print(f"   âœ“ Ã–lÃ§ekleyici hazÄ±r ({SCALING_METODU})")
        except Exception as e:
            print(f"   âš ï¸  Ã–lÃ§ekleyici hazÄ±rlanamadÄ±: {e}")

    def goruntu_isle(self, goruntu_yolu: Union[str, Path]) -> np.ndarray:
        """
        GÃ¶rÃ¼ntÃ¼yÃ¼ eÄŸitim pipeline'Ä±yla aynÄ± ÅŸekilde (goruntu_isleyici.goruntu_isle) iÅŸle.
        """
        goruntu = self.isleyici.goruntu_isle(str(goruntu_yolu))
        if goruntu is None:
            raise ValueError(f"GÃ¶rÃ¼ntÃ¼ iÅŸlenemedi: {goruntu_yolu}")
        return goruntu

    def _ozellikler_from_array(
        self,
        goruntu: np.ndarray,
        dosya_adi: str,
        boyut_bayt: int,
        tam_yol: str,
    ) -> Dict:
        """
        Ä°ÅŸlenmiÅŸ gÃ¶rÃ¼ntÃ¼den (np.ndarray) eÄŸitimdeki ile aynÄ± sayÄ±sal Ã¶zellikleri Ã§Ä±kar.
        """
        piksel_array = goruntu.astype(np.float32)
        yukseklik, genislik = piksel_array.shape
        en_boy_orani = genislik / yukseklik if yukseklik > 0 else 0.0

        ort_yogunluk = float(np.mean(piksel_array))
        std_yogunluk = float(np.std(piksel_array))
        min_yogunluk = float(np.min(piksel_array))
        max_yogunluk = float(np.max(piksel_array))

        p1_yogunluk = float(np.percentile(piksel_array, 1))
        p25_yogunluk = float(np.percentile(piksel_array, 25))
        p50_yogunluk = float(np.percentile(piksel_array, 50))
        p75_yogunluk = float(np.percentile(piksel_array, 75))
        p99_yogunluk = float(np.percentile(piksel_array, 99))

        hist, _ = np.histogram(piksel_array.astype(np.uint8), bins=256, range=(0, 256))
        hist = hist / (hist.sum() + 1e-8)
        hist_nonzero = hist[hist > 0]
        entropi = float(-np.sum(hist_nonzero * np.log2(hist_nonzero))) if hist_nonzero.size > 0 else 0.0

        laplacian = ndimage.laplace(piksel_array)
        kontrast = float(np.var(laplacian))
        homojenlik = 1.0 / (1.0 + std_yogunluk) if std_yogunluk is not None else 0.0
        enerji = float(np.sum(hist ** 2))

        carpiklik = float(skew(piksel_array.flatten()))
        basiklik = float(kurtosis(piksel_array.flatten()))

        grad_y, grad_x = np.gradient(piksel_array)
        gradient_magnitude = np.sqrt(grad_x ** 2 + grad_y ** 2)
        ortalama_gradyan = float(np.mean(gradient_magnitude))
        max_gradyan = float(np.max(gradient_magnitude))

        try:
            from skimage.filters import threshold_otsu

            otsu_esik = float(threshold_otsu(piksel_array))
        except Exception:
            otsu_esik = float(np.mean(piksel_array))

        return {
            "dosya_adi": dosya_adi,
            "boyut_bayt": int(boyut_bayt),
            "genislik": int(genislik),
            "yukseklik": int(yukseklik),
            "en_boy_orani": round(en_boy_orani, 4),
            "piksel_sayisi": int(genislik * yukseklik),
            "ort_yogunluk": round(ort_yogunluk, 2),
            "std_yogunluk": round(std_yogunluk, 2),
            "min_yogunluk": round(min_yogunluk, 2),
            "max_yogunluk": round(max_yogunluk, 2),
            "p1_yogunluk": round(p1_yogunluk, 2),
            "p25_yogunluk": round(p25_yogunluk, 2),
            "medyan_yogunluk": round(p50_yogunluk, 2),
            "p75_yogunluk": round(p75_yogunluk, 2),
            "p99_yogunluk": round(p99_yogunluk, 2),
            "entropi": round(entropi, 4),
            "kontrast": round(kontrast, 2),
            "homojenlik": round(homojenlik, 4),
            "enerji": round(enerji, 4),
            "carpiklik": round(carpiklik, 4),
            "basiklik": round(basiklik, 4),
            "ortalama_gradyan": round(ortalama_gradyan, 2),
            "max_gradyan": round(max_gradyan, 2),
            "otsu_esik": round(otsu_esik, 2),
            "tam_yol": tam_yol,
        }
    
    def ozellik_cikar(self, goruntu_yolu: Union[str, Path]) -> pd.DataFrame:
        """
        GÃ¶rÃ¼ntÃ¼yÃ¼ eÄŸitim pipeline'Ä±yla iÅŸle ve sayÄ±sal Ã¶zellikleri Ã§Ä±kar.
        """
        goruntu_yolu = Path(goruntu_yolu)
        islenmis = self.goruntu_isle(goruntu_yolu)

        boyut_bayt = os.path.getsize(goruntu_yolu) if goruntu_yolu.exists() else 0
        ozellikler = self._ozellikler_from_array(
            islenmis,
            dosya_adi=goruntu_yolu.name,
            boyut_bayt=boyut_bayt,
            tam_yol=str(goruntu_yolu),
        )

        df = pd.DataFrame([ozellikler])

        # Kategorik kolonlarÄ± Ã§Ä±kar
        kategorik = ["dosya_adi", "tam_yol"]
        df_ozellikler = df.drop(columns=[c for c in kategorik if c in df.columns])

        # Ã–lÃ§ekleme
        if self.scaler and self.scaler_columns:
            for col in self.scaler_columns:
                if col not in df_ozellikler.columns:
                    df_ozellikler[col] = 0.0
            df_ozellikler = df_ozellikler[self.scaler_columns]
            scaled = self.scaler.transform(df_ozellikler)
            df_ozellikler = pd.DataFrame(scaled, columns=self.scaler_columns)
        elif self.feature_names:
            eksik = [c for c in self.feature_names if c not in df_ozellikler.columns]
            if eksik:
                raise ValueError(f"Eksik Ã¶zellik kolonlarÄ±: {eksik}")
            df_ozellikler = df_ozellikler[self.feature_names]

        if self.selected_features:
            eksik = [c for c in self.selected_features if c not in df_ozellikler.columns]
            if eksik:
                raise ValueError(f"SeÃ§ilen Ã¶zellikler veri setinde yok: {eksik}")
            df_ozellikler = df_ozellikler[self.selected_features]

        return df_ozellikler
    
    def tahmin_yap(self, goruntu_yolu: Union[str, Path], 
                   detayli: bool = True) -> Dict:
        """
        Tek bir gÃ¶rÃ¼ntÃ¼ iÃ§in demans seviyesi tahmini yap.
        
        Bu fonksiyon, ham MRI gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ alÄ±r ve ÅŸu adÄ±mlarÄ± gerÃ§ekleÅŸtirir:
        1. GÃ¶rÃ¼ntÃ¼yÃ¼ iÅŸle (normalizasyon, yeniden boyutlandÄ±rma, vb.)
        2. Ã–zellikleri Ã§Ä±kar (20+ sayÄ±sal Ã¶zellik)
        3. Model ile tahmin yap
        4. OlasÄ±lÄ±klarÄ± ve gÃ¼ven skorunu hesapla
        
        Ã‡Ä±ktÄ± Ã¶rnekleri:
        {
            'tahmin': 'NonDemented (SaÄŸlÄ±klÄ±)',
            'tahmin_kodu': 0,
            'guven': 0.92,
            'olasiliklar': {
                'NonDemented': 0.92,
                'VeryMildDemented': 0.05,
                'MildDemented': 0.02,
                'ModerateDemented': 0.01
            },
            'goruntu_yolu': '/path/to/image.jpg'
        }
        
        Args:
            goruntu_yolu: MRI gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n dosya yolu
            detayli: True ise tÃ¼m olasÄ±lÄ±klarÄ± da dÃ¶ndÃ¼r
            
        Returns:
            Dict: Tahmin sonuÃ§larÄ± (tahmin, gÃ¼ven, olasÄ±lÄ±klar)
        """
        """
        Tek bir gÃ¶rÃ¼ntÃ¼ iÃ§in tahmin yap.
        
        Args:
            goruntu_yolu: GÃ¶rÃ¼ntÃ¼ dosyasÄ±nÄ±n yolu
            detayli: DetaylÄ± Ã§Ä±ktÄ± (olasÄ±lÄ±klar dahil)
            
        Returns:
            Tahmin sonuÃ§larÄ± sÃ¶zlÃ¼ÄŸÃ¼
        """
        print(f"\nğŸ” Tahmin yapÄ±lÄ±yor: {Path(goruntu_yolu).name}")
        
        # Ã–zellikleri Ã§Ä±kar
        X = self.ozellik_cikar(goruntu_yolu)
        
        # Tahmin yap
        tahmin = self.model.predict(X)[0]
        sinif_adi = self.sinif_isimleri[tahmin]
        
        sonuc = {
            'dosya': str(goruntu_yolu),
            'tahmin_sinif': int(tahmin),
            'tahmin_adi': sinif_adi
        }
        
        # OlasÄ±lÄ±klar (varsa)
        if hasattr(self.model, 'predict_proba'):
            olasiliklar = self.model.predict_proba(X)[0]
            sonuc['olasiliklar'] = {
                self.sinif_isimleri[i]: float(prob) 
                for i, prob in enumerate(olasiliklar)
            }
            sonuc['guven_skoru'] = float(max(olasiliklar))
        
        # Ekrana yazdÄ±r
        print(f"\n{'='*60}")
        print(f"ğŸ“Š TAHMÄ°N SONUCU")
        print(f"{'='*60}")
        print(f"ğŸ¯ Tahmin: {sinif_adi}")
        
        if 'guven_skoru' in sonuc:
            print(f"ğŸ“ˆ GÃ¼ven Skoru: {sonuc['guven_skoru']:.2%}")
            
            if detayli:
                print(f"\nğŸ“‹ SÄ±nÄ±f OlasÄ±lÄ±klarÄ±:")
                for sinif, prob in sorted(sonuc['olasiliklar'].items(), 
                                         key=lambda x: x[1], reverse=True):
                    bar = 'â–ˆ' * int(prob * 40)
                    print(f"   {sinif:40s}: {prob:6.2%} {bar}")
        
        print(f"{'='*60}\n")
        
        return sonuc
    
    def batch_tahmin(self, goruntu_klasoru: Union[str, Path], 
                     kaydet: bool = True) -> List[Dict]:
        """
        Bir klasÃ¶rdeki tÃ¼m gÃ¶rÃ¼ntÃ¼ler iÃ§in toplu tahmin yap.
        
        Bu fonksiyon, klinik kullanÄ±m iÃ§in idealdir:
        - Ã‡ok sayÄ±da hasta gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ tek seferde iÅŸler
        - SonuÃ§larÄ± CSV'ye kaydeder (raporlama iÃ§in)
        - Ä°lerleme Ã§ubuÄŸu gÃ¶sterir
        
        Ã‡Ä±ktÄ± CSV formatÄ±:
        | goruntu_adi | tahmin | tahmin_kodu | guven | NonDemented | VeryMildDemented | ... |
        |-------------|--------|-------------|-------|-------------|------------------|-----|
        | img1.jpg    | NonDemented | 0 | 0.92 | 0.92 | 0.05 | ... |
        
        KullanÄ±m senaryosu:
        ```python
        inferencer = ModelInference('xgboost_model.pkl')
        sonuclar = inferencer.batch_tahmin('./yeni_hastalar/')
        # SonuÃ§lar otomatik CSV'ye kaydedilir
        ```
        
        Args:
            goruntu_klasoru: GÃ¶rÃ¼ntÃ¼lerin bulunduÄŸu klasÃ¶r yolu
            kaydet: SonuÃ§larÄ± CSV'ye kaydet (varsayÄ±lan: True)
            
        Returns:
            List[Dict]: TÃ¼m tahmin sonuÃ§larÄ±
        """
        klasor = Path(goruntu_klasoru)
        
        if not klasor.exists():
            raise FileNotFoundError(f"KlasÃ¶r bulunamadÄ±: {goruntu_klasoru}")
        
        # GÃ¶rÃ¼ntÃ¼leri bul
        gorseller = list(klasor.glob("*.jpg")) + list(klasor.glob("*.png"))
        
        if not gorseller:
            print(f"âš ï¸  KlasÃ¶rde gÃ¶rÃ¼ntÃ¼ bulunamadÄ±: {goruntu_klasoru}")
            return []
        
        print(f"\nâš¡ Batch tahmin: {len(gorseller)} gÃ¶rÃ¼ntÃ¼ (paralel: {self.n_jobs} Ã§ekirdek)")
        print(f"{'='*60}")
        
        # âš¡ Paralel batch tahmin
        partial_func = partial(_batch_tahmin_wrapper, model_yolu=self.model_yolu)
        
        with Pool(processes=self.n_jobs) as pool:
            sonuclar = list(tqdm(
                pool.imap(partial_func, gorseller),
                total=len(gorseller),
                desc="Batch tahmin (paralel)"
            ))
        
        # Ã–zet
        print(f"\n{'='*60}")
        print(f"ğŸ“Š BATCH TAHMÄ°N Ã–ZETÄ°")
        print(f"{'='*60}")
        print(f"Toplam: {len(gorseller)}")
        print(f"BaÅŸarÄ±lÄ±: {len([s for s in sonuclar if 'tahmin_sinif' in s])}")
        print(f"HatalÄ±: {len([s for s in sonuclar if 'hata' in s])}")
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
        if sonuclar:
            sinif_sayilari = {}
            for sonuc in sonuclar:
                if 'tahmin_adi' in sonuc:
                    sinif = sonuc['tahmin_adi']
                    sinif_sayilari[sinif] = sinif_sayilari.get(sinif, 0) + 1
            
            print(f"\nğŸ“ˆ Tahmin DaÄŸÄ±lÄ±mÄ±:")
            for sinif, sayi in sorted(sinif_sayilari.items()):
                print(f"   {sinif:40s}: {sayi:3d}")
        
        # Kaydet
        if kaydet and sonuclar:
            cikti_dosya = klasor / f"tahminler_{Path(self.model_yolu).stem}.csv"
            df = pd.DataFrame(sonuclar)
            df.to_csv(cikti_dosya, index=False, encoding='utf-8')
            print(f"\nğŸ’¾ SonuÃ§lar kaydedildi: {cikti_dosya}")
        
        return sonuclar


def main():
    """Ana fonksiyon."""
    parser = argparse.ArgumentParser(
        description="EÄŸitilmiÅŸ model ile MRI tahmin (inference)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  # Tek gÃ¶rÃ¼ntÃ¼ tahmin
  python3 inference.py --model xgboost_latest.pkl --image test.jpg
  
  # Batch tahmin (klasÃ¶rdeki tÃ¼m gÃ¶rÃ¼ntÃ¼ler)
  python3 inference.py --model xgboost_latest.pkl --batch ./test_images/
  
  # En son eÄŸitilmiÅŸ model ile tahmin
  python3 inference.py --image test.jpg
        """
    )
    
    parser.add_argument(
        '--model',
        type=str,
        help='Model dosyasÄ± yolu (.pkl). Belirtilmezse en son model kullanÄ±lÄ±r.'
    )
    
    parser.add_argument(
        '--image',
        type=str,
        help='Tahmin yapÄ±lacak tek bir gÃ¶rÃ¼ntÃ¼ dosyasÄ±'
    )
    
    parser.add_argument(
        '--batch',
        type=str,
        help='Tahmin yapÄ±lacak gÃ¶rÃ¼ntÃ¼lerin bulunduÄŸu klasÃ¶r'
    )
    
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='Batch tahmin sonuÃ§larÄ±nÄ± kaydetme'
    )
    
    args = parser.parse_args()
    
    # Parametre kontrolÃ¼
    if not args.image and not args.batch:
        parser.error("--image veya --batch belirtilmeli")
    
    # Model yolu
    if args.model:
        model_yolu = args.model
    else:
        # En son model ara
        modeller = sorted(MODELS_KLASORU.glob("*.pkl"), key=lambda p: p.stat().st_mtime)
        if not modeller:
            print("âŒ HiÃ§ model bulunamadÄ±!")
            print(f"   Aranan klasÃ¶r: {MODELS_KLASORU}")
            print(f"\nğŸ’¡ Ã–nce model eÄŸitin:")
            print(f"   python3 train.py --auto")
            return 1
        
        model_yolu = modeller[-1]
        print(f"â„¹ï¸  En son model kullanÄ±lÄ±yor: {model_yolu.name}")
    
    try:
        # Inference nesnesi oluÅŸtur
        inferencer = ModelInference(model_yolu)
        
        # Tek gÃ¶rÃ¼ntÃ¼ veya batch
        if args.image:
            inferencer.tahmin_yap(args.image, detayli=True)
        elif args.batch:
            inferencer.batch_tahmin(args.batch, kaydet=not args.no_save)
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ HATA: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
